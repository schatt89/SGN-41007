{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (pen and paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense1: 15052900\n",
      "Dense2: 10100\n",
      "Dense3: 1010\n",
      "Total: 15064010\n"
     ]
    }
   ],
   "source": [
    "layer1 = (224 * 224 * 3 * 100) + 100\n",
    "print(\"Dense1:\", layer1)\n",
    "layer2 = (100 * 100) + 100\n",
    "print(\"Dense2:\", layer2)\n",
    "layer3 = (100 * 10) + 10\n",
    "print(\"Dense3:\", layer3)\n",
    "\n",
    "total = layer1 + layer2 + layer3\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (pen and paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1: 2432\n",
      "MaxPool: 0\n",
      "Conv2: 25632\n",
      "MaxPool: 0\n",
      "Flatten: 0\n",
      "Dense1: 51300\n",
      "Dense2: 202\n",
      "Total: 79566\n"
     ]
    }
   ],
   "source": [
    "conv1 = (5*5*3*32) + 32\n",
    "print(\"Conv1:\", conv1)\n",
    "print(\"MaxPool:\", 0)\n",
    "conv2 = (5*5*32*32) + 32\n",
    "print(\"Conv2:\", conv2)\n",
    "print(\"MaxPool:\", 0)\n",
    "print(\"Flatten:\", 0)\n",
    "dense1 = (512 * 100) + 100\n",
    "print(\"Dense1:\", dense1)\n",
    "dense2 = (100 * 2) + 2\n",
    "print(\"Dense2:\", dense2)\n",
    "\n",
    "total = conv1 + conv2 + dense1 + dense2\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scalar multiplications on the first layer: 9830400\n"
     ]
    }
   ],
   "source": [
    "# How many scalar multiplications take place on teh first convolutional layer?\n",
    "n_mult = (64 * 64 * 32) * (5 * 5 * 3)\n",
    "print(\"Number of scalar multiplications on the first layer:\", n_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.2717 - accuracy: 0.1771 - val_loss: 2.2286 - val_accuracy: 0.3825\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 2.2043 - accuracy: 0.2786 - val_loss: 2.1440 - val_accuracy: 0.4982\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 2.1153 - accuracy: 0.3696 - val_loss: 2.0279 - val_accuracy: 0.5831\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.9953 - accuracy: 0.4521 - val_loss: 1.8740 - val_accuracy: 0.6675\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1.8456 - accuracy: 0.5153 - val_loss: 1.6869 - val_accuracy: 0.7169\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.6712 - accuracy: 0.5679 - val_loss: 1.4772 - val_accuracy: 0.7502\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.4953 - accuracy: 0.6061 - val_loss: 1.2726 - val_accuracy: 0.7716\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1.3345 - accuracy: 0.6385 - val_loss: 1.0942 - val_accuracy: 0.7924\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1.2028 - accuracy: 0.6627 - val_loss: 0.9503 - val_accuracy: 0.8091\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.0940 - accuracy: 0.6834 - val_loss: 0.8395 - val_accuracy: 0.8217\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.0081 - accuracy: 0.7035 - val_loss: 0.7541 - val_accuracy: 0.8322\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.9382 - accuracy: 0.7217 - val_loss: 0.6880 - val_accuracy: 0.8417\n",
      "Test loss: 0.6879789583683014\n",
      "Test accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "base_model = tf.keras.applications.mobilenet.MobileNet(\n",
    "    input_shape = (128,128,3),\n",
    "    include_top = False,\n",
    "    alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = base_model.inputs[0] # Grab the input of base model\n",
    "out_tensor = base_model.outputs[0] # Grab the output of base model\n",
    "\n",
    "\n",
    "out_tensor = tf.keras.layers.Flatten()(out_tensor)\n",
    "\n",
    "out_tensor = tf.keras.layers.Dense(units = 100, activation = \"relu\")(out_tensor)\n",
    "\n",
    "out_tensor = tf.keras.layers.Dense(units = 10, activation = \"softmax\")(out_tensor)\n",
    "\n",
    "# Define the full model by the endpoints.\n",
    "model = tf.keras.models.Model(inputs = [in_tensor], outputs = [out_tensor])\n",
    "#model.summary()\n",
    "\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.backend import resize_images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "def resize(x, target_shape=(128, 128)):\n",
    "    N, W, H = x.shape\n",
    "    x_resized = [None] * N\n",
    "    for i in range(x.shape[0]):\n",
    "        x_resized[i] = cv2.resize(x[i], target_shape, interpolation=cv2.INTER_CUBIC)\n",
    "    x_resized = np.array(x_resized)\n",
    "    x_resized = x_resized[:, :, :, np.newaxis]\n",
    "    x_resized = np.tile(x_resized, (1, 1, 1, 3))\n",
    "    return x_resized\n",
    "    \n",
    "x_train_resized = resize(x_train)\n",
    "x_test_resized = resize(x_test)\n",
    "\n",
    "x_train_resized /= 255\n",
    "x_test_resized /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 33s 558us/sample - loss: 0.1285 - accuracy: 0.9604 - val_loss: 0.1170 - val_accuracy: 0.9617\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 30s 495us/sample - loss: 0.1196 - accuracy: 0.9631 - val_loss: 0.1098 - val_accuracy: 0.9637\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 30s 499us/sample - loss: 0.1114 - accuracy: 0.9649 - val_loss: 0.1038 - val_accuracy: 0.9665\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 31s 517us/sample - loss: 0.1044 - accuracy: 0.9676 - val_loss: 0.0983 - val_accuracy: 0.9680\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 31s 517us/sample - loss: 0.0990 - accuracy: 0.9693 - val_loss: 0.0934 - val_accuracy: 0.9695\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 30s 508us/sample - loss: 0.0939 - accuracy: 0.9703 - val_loss: 0.0896 - val_accuracy: 0.9702\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 31s 519us/sample - loss: 0.0902 - accuracy: 0.9714 - val_loss: 0.0861 - val_accuracy: 0.9713\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 30s 505us/sample - loss: 0.0854 - accuracy: 0.9732 - val_loss: 0.0827 - val_accuracy: 0.9726\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 30s 507us/sample - loss: 0.0821 - accuracy: 0.9739 - val_loss: 0.0798 - val_accuracy: 0.9734\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 0.0787 - accuracy: 0.9752 - val_loss: 0.0773 - val_accuracy: 0.9746\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 30s 505us/sample - loss: 0.0753 - accuracy: 0.9762 - val_loss: 0.0747 - val_accuracy: 0.9756\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 30s 505us/sample - loss: 0.0719 - accuracy: 0.9775 - val_loss: 0.0726 - val_accuracy: 0.9762\n",
      "Test loss: 0.07259189371634275\n",
      "Test accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_resized, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,validation_data=(x_test_resized, y_test))\n",
    "\n",
    "score = model.evaluate(x_test_resized, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
